#!/usr/bin/env python3
# polyglot_rpg/main.py

import typer
from rich.console import Console
from rich.prompt import Prompt, Confirm
from openai import OpenAI
from markdown_it import MarkdownIt
from tqdm import tqdm
import re
from pathlib import Path
import yaml
import json
from markdown_it.token import Token
import tiktoken
import hashlib
import importlib.resources  # –î–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ —Ñ–∞–π–ª–∞–º –¥–∞–Ω–Ω—ã—Ö –≤–Ω—É—Ç—Ä–∏ –ø–∞–∫–µ—Ç–∞
from typing import List, Dict, Optional, Any

from markdownify import markdownify as md_from_html

app = typer.Typer(pretty_exceptions_show_locals=False, add_completion=False)
console = Console()

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞ ---
CONFIG_NAME = "01_config.yaml"
INPUT_DIR_NAME = "02_input_chapters"
WORKSPACE_DIR_NAME = "03_translation_workspace"
AST_DIR_NAME = "1_asts"
GLOSSARY_REVIEW_NAME = "2_glossary.for_review.yaml"
GLOSSARY_FINAL_NAME = "2_glossary.final.yaml"
TRANSLATED_AST_DIR_NAME = "3_translated_asts"
FINAL_DIR_NAME = "4_final_chapters"
CACHE_DIR_NAME = ".cache"
DEFAULT_CONFIG_TEMPLATE_NAME = "default_config_template.yaml"

# --- –ö–ª–∞—Å—Å—ã –¥–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –ª–æ–≥–∏–∫–∏ ---

class Project:
    """–ò–Ω–∫–∞–ø—Å—É–ª–∏—Ä—É–µ—Ç –ø—É—Ç–∏ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–æ–µ–∫—Ç–∞."""
    def __init__(self, project_dir: Path):
        self.project_dir = project_dir.resolve()
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.input_dir = self.project_dir / INPUT_DIR_NAME
        self.workspace_dir = self.project_dir / WORKSPACE_DIR_NAME
        self.ast_dir = self.workspace_dir / AST_DIR_NAME
        self.translated_ast_dir = self.workspace_dir / TRANSLATED_AST_DIR_NAME
        self.final_dir = self.workspace_dir / FINAL_DIR_NAME
        self.cache_dir = self.workspace_dir / CACHE_DIR_NAME
        
        # –§–∞–π–ª—ã
        self.config_path = self.project_dir / CONFIG_NAME
        self.glossary_review_path = self.workspace_dir / GLOSSARY_REVIEW_NAME
        self.glossary_final_path = self.workspace_dir / GLOSSARY_FINAL_NAME
        
        self.config = self._load_config()

    def _load_config(self) -> Dict[str, Any]:
        if not self.config_path.exists():
            console.print(f"[bold red]–û—à–∏–±–∫–∞: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.config_path}[/bold red]")
            raise typer.Exit(1)
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def get_source_files(self) -> List[Path]:
        return sorted(list(self.input_dir.glob("*.md")))

class TokenCounter:
    """–ü—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Å –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –æ—Ü–µ–Ω–∫–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏."""
    def __init__(self, model_name: str = "gpt-4o"):
        self.input_tokens = 0
        self.output_tokens = 0
        try:
            self.encoding = tiktoken.encoding_for_model(model_name)
        except KeyError:
            self.encoding = tiktoken.get_encoding("cl100k_base")
        self.input_cost_per_m = 5.0
        self.output_cost_per_m = 15.0

    def _count(self, text: str) -> int:
        return len(self.encoding.encode(text, disallowed_special=()))

    def add_input(self, text: str):
        self.input_tokens += self._count(text)

    def add_output(self, text: str):
        self.output_tokens += self._count(text)

    def report(self, command_name: str):
        total_input_cost = (self.input_tokens / 1_000_000) * self.input_cost_per_m
        total_output_cost = (self.output_tokens / 1_000_000) * self.output_cost_per_m
        total_cost = total_input_cost + total_output_cost
        
        console.print("\n[bold]üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤[/bold]")
        console.print(f"–ö–æ–º–∞–Ω–¥–∞: [cyan]{command_name}[/cyan]")
        console.print(f"   –¢–æ–∫–µ–Ω—ã –Ω–∞ –≤—Ö–æ–¥ (input):  [green]{self.input_tokens:,}[/green]")
        console.print(f"   –¢–æ–∫–µ–Ω—ã –Ω–∞ –≤—ã—Ö–æ–¥ (output): [green]{self.output_tokens:,}[/green]")
        console.print(f"   [bold]–ò—Ç–æ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤:[/bold]          [bold green]{(self.input_tokens + self.output_tokens):,}[/bold green]")
        console.print(f"   [yellow]–ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å (GPT-4o):[/yellow] [bold yellow]${total_cost:.4f}[/bold yellow]")

class TranslationCache:
    """–£–ø—Ä–∞–≤–ª—è–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ API –≤—ã–∑–æ–≤–æ–≤."""
    def __init__(self, cache_dir: Path):
        self.cache_path = cache_dir / "translations_cache.json"
        self.cache_dir = cache_dir
        self.cache = self._load()

    def _load(self) -> Dict[str, str]:
        self.cache_dir.mkdir(exist_ok=True)
        if not self.cache_path.exists():
            return {}
        try:
            with open(self.cache_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            return {}

    def _get_hash(self, text: str) -> str:
        return hashlib.sha256(text.encode('utf-8')).hexdigest()

    def get(self, text: str) -> Optional[str]:
        return self.cache.get(self._get_hash(text))

    def set(self, original_text: str, translated_text: str):
        self.cache[self._get_hash(original_text)] = translated_text

    def save(self):
        with open(self.cache_path, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, indent=2, ensure_ascii=False)

class Glossary:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–æ—Å—Ç—É–ø –∫ —Ç–µ—Ä–º–∏–Ω–∞–º –∏–∑ –≥–ª–æ—Å—Å–∞—Ä–∏—è."""
    def __init__(self, path: Path):
        self.path = path
        self.terms = self._load()

    def _load(self) -> Dict[str, str]:
        if not self.path.exists():
            console.print("üìñ –ì–ª–æ—Å—Å–∞—Ä–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–µ—Ä–µ–≤–æ–¥ –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω –±–µ–∑ –Ω–µ–≥–æ.")
            return {}
        
        console.print(f"üìñ –ù–∞–π–¥–µ–Ω –≥–ª–æ—Å—Å–∞—Ä–∏–π: [green]{self.path}[/green]")
        with open(self.path, 'r', encoding='utf-8') as f:
            glossary_data = yaml.safe_load(f)
        
        if not glossary_data:
            console.print(f"   [yellow]–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ì–ª–æ—Å—Å–∞—Ä–∏–π –ø—É—Å—Ç.[/yellow]")
            return {}
            
        terms = {item['term']: item['translation'] for item in glossary_data if item.get('term') and item.get('translation')}
        if terms:
            console.print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(terms)} —Ç–µ—Ä–º–∏–Ω–æ–≤.")
        else:
            console.print(f"   [yellow]–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ì–ª–æ—Å—Å–∞—Ä–∏–π –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π.[/yellow]")
        return terms

    def apply_to_text(self, text: str) -> str:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –≥–ª–æ—Å—Å–∞—Ä–∏–π –∫ —Ç–µ–∫—Å—Ç—É, –∑–∞–º–µ–Ω—è—è —Ç–µ—Ä–º–∏–Ω—ã."""
        if not self.terms:
            return text
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ—Ä–º–∏–Ω—ã –æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —á–∞—Å—Ç–∏—á–Ω—ã—Ö –∑–∞–º–µ–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, "Mage Hand" –ø–µ—Ä–µ–¥ "Mage")
        for term, translation in sorted(self.terms.items(), key=lambda i: len(i[0]), reverse=True):
             # –ò—Å–ø–æ–ª—å–∑—É–µ–º regex –¥–ª—è –∑–∞–º–µ–Ω—ã —Ü–µ–ª—ã—Ö —Å–ª–æ–≤, —á—Ç–æ–±—ã –Ω–µ –∑–∞–º–µ–Ω—è—Ç—å 'cat' –≤ 'caterpillar'
            text = re.sub(r'\b' + re.escape(term) + r'\b', translation, text, flags=re.IGNORECASE)
        return text

class Translator:
    """–£–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ–º –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é LLM, –∫—ç—à–∞ –∏ –≥–ª–æ—Å—Å–∞—Ä–∏—è."""
    def __init__(self, project: Project):
        self.project = project
        api_conf = project.config['api']
        self.client = OpenAI(base_url=api_conf['url'], api_key=api_conf['key'])
        self.model = api_conf['model']
        self.temperature = api_conf.get('temperature', 0.2)
        
        self.token_counter = TokenCounter(model_name=self.model)
        self.cache = TranslationCache(project.cache_dir)
        self.glossary = Glossary(project.glossary_final_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        try:
            self.system_prompt = project.config['translation_settings']['system_prompt']
        except KeyError:
            console.print("[bold red]–û—à–∏–±–∫–∞: 'system_prompt' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ 'translation_settings' –≤ —Ñ–∞–π–ª–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.[/bold red]")
            raise typer.Exit(1)

        self.failure_phrases = ("please provide the english text", "i'm ready when you are", "i need the english text")

    def translate_chunk(self, chunk: str) -> str:
        """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç –æ–¥–∏–Ω —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è –∫—ç—à –∏ –≥–ª–æ—Å—Å–∞—Ä–∏–π."""
        cached = self.cache.get(chunk)
        if cached:
            return cached

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≥–ª–æ—Å—Å–∞—Ä–∏–π –∫ —Ç–µ–∫—Å—Ç—É –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π –≤ LLM
        text_to_translate = self.glossary.apply_to_text(chunk)
        
        # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –≤ —Ç–µ–≥–∏ <data>
        user_message_content = f"<data>\n{text_to_translate}\n</data>"

        try:
            self.token_counter.add_input(self.system_prompt)
            self.token_counter.add_input(user_message_content)
            
            resp = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": user_message_content}
                ],
                temperature=self.temperature,
            )
            raw_translation = resp.choices[0].message.content
            self.token_counter.add_output(raw_translation)

            if any(phrase in raw_translation.lower() for phrase in self.failure_phrases):
                console.print(f"\n[bold yellow]–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –º–æ–¥–µ–ª—å –æ—Ç–∫–∞–∑–∞–ª–∞—Å—å –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ—Ä–∏–≥–∏–Ω–∞–ª.[/bold yellow]")
                translation = chunk # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —á–∞–Ω–∫, –Ω–µ –ø—Ä–µ–¥-–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π
            else:
                translation = re.sub(r"<think>.*?</think>", "", raw_translation, flags=re.DOTALL).strip()
            
            self.cache.set(chunk, translation)
            return translation

        except Exception as e:
            console.print(f"\n[yellow]–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —á–∞–Ω–∫ '{chunk[:30]}...'. ({e})[/yellow]")
            return chunk # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏

    def save_cache(self):
        self.cache.save()

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---

def _tokens_to_json(tokens: List[Token]) -> List[Dict]:
    """–°–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç —Ç–æ–∫–µ–Ω—ã –≤ JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç."""
    return [t.as_dict() for t in tokens]

def _extract_strings_from_json(data) -> list[str]:
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –ª—é–±–æ–π –≤–ª–æ–∂–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã."""
    found_strings = []
    if isinstance(data, dict):
        for value in data.values():
            found_strings.extend(_extract_strings_from_json(value))
    elif isinstance(data, list):
        for item in data:
            if isinstance(item, str):
                found_strings.append(item)
            else:
                found_strings.extend(_extract_strings_from_json(item))
    return found_strings

# --- –ö–æ–º–∞–Ω–¥—ã CLI ---

@app.command()
def init(project_dir: Path = typer.Argument(..., help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞.")):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞."""
    console.print(f"‚ñ∂Ô∏è  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –≤ [cyan]{project_dir}[/cyan]...")
    project_dir.mkdir(exist_ok=True)
    (project_dir / INPUT_DIR_NAME).mkdir(exist_ok=True)
    workspace = project_dir / WORKSPACE_DIR_NAME
    workspace.mkdir(exist_ok=True)
    (workspace / AST_DIR_NAME).mkdir(exist_ok=True)
    (workspace / TRANSLATED_AST_DIR_NAME).mkdir(exist_ok=True)
    (workspace / FINAL_DIR_NAME).mkdir(exist_ok=True)
    (workspace / CACHE_DIR_NAME).mkdir(exist_ok=True)
    (project_dir / INPUT_DIR_NAME / ".gitkeep").touch()
    
    config_path = project_dir / CONFIG_NAME
    if not config_path.exists():
        # –ß–∏—Ç–∞–µ–º —à–∞–±–ª–æ–Ω –∏–∑ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø–∞–∫–µ—Ç–∞ –∏ –∫–æ–ø–∏—Ä—É–µ–º –µ–≥–æ –≤ –ø—Ä–æ–µ–∫—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        try:
            # –ù–∞—Ö–æ–¥–∏–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —à–∞–±–ª–æ–Ω–∞ –≤–Ω—É—Ç—Ä–∏ –Ω–∞—à–µ–≥–æ –ø–∞–∫–µ—Ç–∞ ('polyglot_rpg')
            template_path = importlib.resources.files('polyglot_rpg').joinpath(DEFAULT_CONFIG_TEMPLATE_NAME)
            # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —à–∞–±–ª–æ–Ω–∞
            template_content = template_path.read_text(encoding='utf-8')
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ –Ω–æ–≤—ã–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª
            config_path.write_text(template_content, encoding='utf-8')
            console.print(f"üìÑ –°–æ–∑–¥–∞–Ω –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª: [green]{config_path}[/green]")
        except FileNotFoundError:
            console.print(f"[bold red]–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: –§–∞–π–ª —à–∞–±–ª–æ–Ω–∞ '{DEFAULT_CONFIG_TEMPLATE_NAME}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤–Ω—É—Ç—Ä–∏ –ø–∞–∫–µ—Ç–∞.[/bold red]")
            raise typer.Exit(1)
    else:
        console.print(f"üìÑ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: [yellow]{config_path}[/yellow]")
    
    console.print(f"\n[bold green]‚úÖ –ü—Ä–æ–µ–∫—Ç '{project_dir.name}' —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω![/bold green]")
    console.print(f"‚û°Ô∏è  –¢–µ–ø–µ—Ä—å –ø–æ–º–µ—Å—Ç–∏—Ç–µ –≤–∞—à–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ .md —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é [cyan]{project_dir / INPUT_DIR_NAME}[/cyan]")


@app.command()
def create_glossary(
    project_dir: Path = typer.Argument(..., help="–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞.", exists=True),
    use_llm: bool = typer.Option(False, "--use-llm", help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ç–µ—Ä–º–∏–Ω–æ–≤."),
    pre_translate: bool = typer.Option(False, "--pre-translate", help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤ (—Ç—Ä–µ–±—É–µ—Ç --use-llm)."),
):
    """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –∏ —Å–æ–∑–¥–∞–µ—Ç —á–µ—Ä–Ω–æ–≤–∏–∫ –≥–ª–æ—Å—Å–∞—Ä–∏—è."""
    if pre_translate and not use_llm:
        console.print("[bold red]–û—à–∏–±–∫–∞: –û–ø—Ü–∏—è --pre-translate –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –≤–º–µ—Å—Ç–µ —Å --use-llm.[/bold red]")
        raise typer.Exit(1)
        
    console.print(f"‚ñ∂Ô∏è  –ó–∞–ø—É—Å–∫ —Å–æ–∑–¥–∞–Ω–∏—è –≥–ª–æ—Å—Å–∞—Ä–∏—è –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ [cyan]{project_dir.name}[/cyan]")
    
    project = Project(project_dir)
    glossary_path = project.glossary_review_path
    
    source_files = project.get_source_files()
    if not source_files:
        console.print(f"[bold yellow]–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ .md —Ñ–∞–π–ª–æ–≤ –≤ {project.input_dir}[/bold yellow]")
        raise typer.Exit()

    all_terms = set()
    translations_map = {}

    if use_llm:
        console.print("ü§ñ [bold]–†–µ–∂–∏–º LLM –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω.[/bold] –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è.")
        try:
            api_conf = project.config['api']
            glossary_conf = project.config['glossary_settings']
            extraction_prompt = glossary_conf['extraction_prompt']
            filtering_prompt = glossary_conf['filtering_prompt']
            translation_prompt = glossary_conf['translation_prompt']
            client = OpenAI(base_url=api_conf['url'], api_key=api_conf['key'])
            token_counter = TokenCounter(model_name=api_conf['model'])
        except KeyError as e:
            console.print(f"[bold red]–û—à–∏–±–∫–∞ –≤ —Ñ–∞–π–ª–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–ª—é—á –∏–ª–∏ —Ä–∞–∑–¥–µ–ª: {e}.[/bold red]")
            console.print("[bold red]–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤–∞—à config.yaml —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–∞–∑–¥–µ–ª—ã 'api' –∏ 'glossary_settings' —Å–æ –≤—Å–µ–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏.[/bold red]")
            raise typer.Exit(code=1)

        # –≠—Ç–∞–ø 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ
        console.print("üß† [–≠—Ç–∞–ø 1/3] –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM...")
        for source_path in tqdm(source_files, desc="–ê–Ω–∞–ª–∏–∑ –≥–ª–∞–≤"):
            source_text = source_path.read_text(encoding='utf-8')
            if not source_text.strip(): continue
            try:
                user_content = f"<data>\n{source_text}\n</data>"
                token_counter.add_input(extraction_prompt)
                token_counter.add_input(user_content)
                
                resp = client.chat.completions.create(
                    model=api_conf['model'], messages=[{"role": "system", "content": extraction_prompt}, {"role": "user", "content": user_content}],
                    temperature=0.0, response_format={"type": "json_object"},
                )
                response_content = resp.choices[0].message.content
                token_counter.add_output(response_content)
                
                content = json.loads(response_content)
                found_terms = _extract_strings_from_json(content)
                all_terms.update(found_terms)
            except Exception as e:
                console.print(f"\n[yellow]–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–∑ {source_path.name}. ({e})[/yellow]")

        # –≠—Ç–∞–ø 2: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        console.print(f"\nüîç [–≠—Ç–∞–ø 2/3] –ù–∞–π–¥–µ–Ω–æ {len(all_terms)} —É–Ω–∏–∫. —Ç–µ—Ä–º–∏–Ω–æ–≤. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è...")
        if all_terms:
            try:
                term_list_json = json.dumps({"terms": sorted(list(all_terms))})
                user_content = f"<data>\n{term_list_json}\n</data>"
                token_counter.add_input(filtering_prompt)
                token_counter.add_input(user_content)

                resp = client.chat.completions.create(
                    model=api_conf['model'], messages=[{"role": "system", "content": filtering_prompt}, {"role": "user", "content": user_content}],
                    temperature=0.0, response_format={"type": "json_object"},
                )
                response_content = resp.choices[0].message.content
                token_counter.add_output(response_content)

                content = json.loads(response_content)
                final_terms = _extract_strings_from_json(content)
                all_terms = set(final_terms)
            except Exception as e:
                console.print(f"\n[yellow]–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏. ({e})[/yellow]")
        
        # –≠—Ç–∞–ø 3: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥
        if pre_translate and all_terms:
            console.print(f"\nüá∑üá∫ [–≠—Ç–∞–ø 3/3] –ù–∞–π–¥–µ–Ω–æ {len(all_terms)} —Ç–µ—Ä–º–∏–Ω–æ–≤. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥...")
            try:
                term_list_json = json.dumps(sorted(list(all_terms)))
                user_content = f"<data>\n{term_list_json}\n</data>"
                token_counter.add_input(translation_prompt)
                token_counter.add_input(user_content)

                resp = client.chat.completions.create(
                    model=api_conf['model'], messages=[{"role": "system", "content": translation_prompt}, {"role": "user", "content": user_content}],
                    temperature=0.1, response_format={"type": "json_object"},
                )
                response_content = resp.choices[0].message.content
                token_counter.add_output(response_content)

                translations_map = json.loads(response_content)
                if not isinstance(translations_map, dict):
                    console.print(f"\n[yellow]–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: LLM –≤–µ—Ä–Ω—É–ª–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–æ–≤ (–Ω–µ —Å–ª–æ–≤–∞—Ä—å).[/yellow]")
                    translations_map = {}
            except Exception as e:
                console.print(f"\n[yellow]–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ. ({e})[/yellow]")
        
        token_counter.report("–°–æ–∑–¥–∞–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è")

    else: # –†–µ–∂–∏–º Regex
        console.print("‚ö° [bold]–†–µ–∂–∏–º Regex –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω (–±—ã—Å—Ç—Ä—ã–π –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑).[/bold]")
        emphasis_regex = re.compile(r'\*\*(.*?)\*\*|\*(.*?)\*')
        proper_noun_regex = re.compile(r'\b[A-Z][a-z]+(?:\s[A-Z][a-z]+)+\b')
        single_cap_word_regex = re.compile(r'(?<!\.\s)\b[A-Z][a-zA-Z\']{3,}\b')
        temp_terms = set()
        for source_path in tqdm(source_files, desc="–ê–Ω–∞–ª–∏–∑ –≥–ª–∞–≤"):
            source_text = source_path.read_text(encoding='utf-8')
            for line in source_text.splitlines():
                for regex in [emphasis_regex, proper_noun_regex, single_cap_word_regex]:
                    for result in regex.findall(line):
                        term = result if isinstance(result, str) else next((s for s in result if s), None)
                        if term and len(term) > 2 and term.lower() not in ["the", "and", "for", "with"]:
                            temp_terms.add(term.strip())
        all_terms = temp_terms

    if not all_terms:
        console.print("[bold yellow]–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –¥–ª—è –≥–ª–æ—Å—Å–∞—Ä–∏—è.[/bold yellow]")
        raise typer.Exit()

    console.print(f"\nüìù –ù–∞–π–¥–µ–Ω–æ {len(all_terms)} —Ç–µ—Ä–º–∏–Ω–æ–≤. –ü–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞...")
    potential_terms_with_context = {}
    full_text = "\n".join([p.read_text(encoding='utf-8') for p in source_files])
    
    for term in tqdm(sorted(list(all_terms)), desc="–ü–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"):
        context_regex = re.compile(r'([^\n]*\b' + re.escape(term) + r'\b[^\n]*)', re.IGNORECASE)
        match = context_regex.search(full_text)
        context = match.group(1).strip() if match else "–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω."
        potential_terms_with_context[term] = (context[:200] + '...') if len(context) > 200 else context

    glossary_for_review = [
        {"term": term, "translation": translations_map.get(term, ""), "context": context}
        for term, context in sorted(potential_terms_with_context.items())
    ]
    header = ("# –≠—Ç–æ—Ç —Ñ–∞–π–ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏...\n")
    with open(glossary_path, 'w', encoding='utf-8') as f:
        f.write(header)
        yaml.dump(glossary_for_review, f, allow_unicode=True, sort_keys=False, width=120)

    console.print(f"\n[bold green]‚úÖ –ß–µ—Ä–Ω–æ–≤–∏–∫ –≥–ª–æ—Å—Å–∞—Ä–∏—è —Å–æ–∑–¥–∞–Ω![/bold green]")
    console.print(f"üìÑ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: [green]{glossary_path}[/green]")
    console.print("‚û°Ô∏è  [bold]–í–∞—à–∏ –¥–µ–π—Å—Ç–≤–∏—è:[/bold] –û—Ç–∫—Ä–æ–π—Ç–µ —Ñ–∞–π–ª, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–µ–≤–æ–¥—ã –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ –∫–∞–∫ '2_glossary.final.yaml'.")


# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–±–æ—Ä–∫–∏ Markdown –∏–∑ inline —Ç–æ–∫–µ–Ω–æ–≤
def build_markdown_from_inline(tokens: List[Token]) -> str:
    """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç—Ä–æ–∫—É Markdown –∏–∑ —Å–ø–∏—Å–∫–∞ inline —Ç–æ–∫–µ–Ω–æ–≤ –∏ –∏—Ö –¥–æ—á–µ—Ä–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤."""
    content = ""
    for token in tokens:
        if token.type == 'text':
            content += token.content
        elif token.type == 'strong_open':
            content += '**'
        elif token.type == 'strong_close':
            content += '**'
        elif token.type == 'em_open':
            content += '*'
        elif token.type == 'em_close':
            content += '*'
        elif token.type == 's_open':
            content += '~~'
        elif token.type == 's_close':
            content += '~~'
        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –¥—Ä—É–≥–∏—Ö inline-—ç–ª–µ–º–µ–Ω—Ç–æ–≤, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    return content

@app.command()
def translate(project_dir: Path = typer.Argument(..., help="–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞.", exists=True)):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞."""
    console.print(f"‚ñ∂Ô∏è  –ó–∞–ø—É—Å–∫ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ [cyan]{project_dir.name}[/cyan]")
    
    project = Project(project_dir)
    translator = Translator(project)
    
    all_source_files = project.get_source_files()
    if not all_source_files:
        console.print(f"[bold yellow]–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ .md —Ñ–∞–π–ª–æ–≤ –≤ {project.input_dir}[/bold yellow]")
        raise typer.Exit()

    # --- –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä —Ñ–∞–π–ª–æ–≤ ---
    console.print("\n[bold]–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª—ã –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞:[/bold]")
    for i, file_path in enumerate(all_source_files):
        console.print(f"  [cyan]{i + 1}[/cyan]: {file_path.name}")
    console.print("  [cyan]all[/cyan]: –ü–µ—Ä–µ–≤–µ—Å—Ç–∏ –≤—Å–µ —Ñ–∞–π–ª—ã")
    
    choice = Prompt.ask("–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä–∞ —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1,3,5) –∏–ª–∏ 'all'", default="all")
    
    files_to_process = []
    if choice.lower() == 'all':
        files_to_process = all_source_files
    else:
        try:
            indices = [int(i.strip()) - 1 for i in choice.split(',')]
            files_to_process = [all_source_files[i] for i in indices if 0 <= i < len(all_source_files)]
        except (ValueError, IndexError):
            console.print("[bold red]–û—à–∏–±–∫–∞: –ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —á–∏—Å–ª–∞ –∏–ª–∏ 'all'.[/bold red]")
            raise typer.Exit(1)
            
    if not files_to_process:
        console.print("[yellow]–ù–µ –≤—ã–±—Ä–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.[/yellow]")
        raise typer.Exit()
        
    console.print("\n[green]–ë—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ —Ñ–∞–π–ª—ã:[/green]")
    for file_path in files_to_process:
        console.print(f"  - {file_path.name}")

    if not Confirm.ask("\n–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å?", default=True):
        raise typer.Exit()

    # --- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–µ—Ä–µ–≤–æ–¥–∞ ---
    md = MarkdownIt("commonmark").enable(["table", "strikethrough"])

    for source_path in tqdm(files_to_process, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–ª–∞–≤"):
        chapter_name = source_path.stem
        console.print(f"\n[bold]--- –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–ª–∞–≤—ã: {chapter_name} ---[/bold]")
        
        source_text = source_path.read_text(encoding='utf-8')
        tokens = md.parse(source_text)
        
        ast_path = project.ast_dir / f"{chapter_name}.ast.json"
        with open(ast_path, 'w', encoding='utf-8') as f:
            json.dump(_tokens_to_json(tokens), f, indent=2, ensure_ascii=False)
        console.print(f"üíæ –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π AST —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ [green]{ast_path}[/green]")

        console.print("üß† –ü–µ—Ä–µ–≤–æ–¥ –±–ª–æ–∫–æ–≤...")
        
        # --- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º ---
        with tqdm(total=len(tokens), desc=f"–ü–µ—Ä–µ–≤–æ–¥ {chapter_name}", leave=False) as pbar:
            i = 0
            while i < len(tokens):
                token = tokens[i]
                pbar.update(1)
                
                parent_type = tokens[i-1].type if i > 0 else ''
                
                is_translatable_content = (
                    token.type == 'inline' and
                    parent_type in [
                        'paragraph_open', 'heading_open', 'list_item_open',
                        'th_open', 'td_open'
                    ]
                )

                if is_translatable_content:
                    text_to_translate = build_markdown_from_inline(token.children)
                    
                    if text_to_translate.strip():
                        translated_text = translator.translate_chunk(text_to_translate)
                        parsed_inline_tokens = md.parseInline(translated_text.strip(), {})
                        
                        if parsed_inline_tokens and parsed_inline_tokens[0].children:
                            token.children = parsed_inline_tokens[0].children
                            token.content = translated_text
                        else:
                            text_token = Token('text', '', 0)
                            text_token.content = translated_text
                            token.children = [text_token]
                            token.content = translated_text

                elif token.type == 'fence':
                    original_content = token.content.strip()
                    if original_content:
                        translated_content = translator.translate_chunk(original_content)
                        token.content = translated_content + '\n'
                
                i += 1

        translated_ast_path = project.translated_ast_dir / f"{chapter_name}.translated.ast.json"
        with open(translated_ast_path, 'w', encoding='utf-8') as f:
            json.dump(_tokens_to_json(tokens), f, indent=2, ensure_ascii=False)
        console.print(f"üíæ –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π AST —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ [green]{translated_ast_path}[/green]")

        console.print("üìù –†–µ–Ω–¥–µ—Ä–∏–Ω–≥ –≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π Markdown...")
        md_out = md.renderer.render(tokens, md.options, {})
        final_md = md_from_html(md_out, heading_style="ATX")
        final_md = re.sub(r"\n{3,}", "\n\n", final_md)
        
        final_path = project.final_dir / f"{source_path.name}"
        final_path.write_text(final_md, encoding='utf-8')
        console.print(f"‚úÖ –ì–ª–∞–≤–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ [bold green]{final_path}[/bold green]")

    translator.save_cache()
    console.print("\nüíæ –ö—ç—à –ø–µ—Ä–µ–≤–æ–¥–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω.")
    console.print("\n[bold green]üéâ –í—ã–±—Ä–∞–Ω–Ω—ã–µ –≥–ª–∞–≤—ã —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã! –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω.[/bold green]")
    translator.token_counter.report("–ü–µ—Ä–µ–≤–æ–¥ –≥–ª–∞–≤")

if __name__ == "__main__":
    app()